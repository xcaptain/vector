#                                    __   __  __  
#                                    \ \ / / / /
#                                     \ V / / /
#                                      \_/  \/
#
#                                    V E C T O R
#                                      Metadata
#
# ------------------------------------------------------------------------------
#
# This file contains structured metadata about the Vector project as a whole.
# It is the source of truth used to generate docs, config examples, and more.
# Once changed, please run to sync everything:
#
#     make generate
#

# ------------------------------------------------------------------------------
# global
# ------------------------------------------------------------------------------
# The global section represents global configuration options.
[options.data_dir]
type = "string"
examples = ["/var/lib/vector"]
null = true
description = """\
The directory used for persisting Vector state, such as on-disk buffers, \
file checkpoints, and more. Please make sure the Vector project has write \
permissions to this dir.\
"""

# ------------------------------------------------------------------------------
# sources.file
# ------------------------------------------------------------------------------
[sources.file]
beta = true
delivery_guarantee = "best_effort"
guides = []
output_types = ["log"]
resources = []
through_description = "one or more local files"

[sources.file.options.data_dir]
type = "string"
examples = ["/var/lib/vector"]
null = true
description = """\
The directory used to persist file checkpoint positions. By default, the \
global `data_dir` is used. Please make sure the Vector project has write \
permissions to this dir.\
"""

[sources.file.options.include]
type = "[string]"
examples = [["/var/log/nginx/*.log"]]
null = false
description = """\
Array of file patterns to include. [Globbing](#globbing) is supported.\
"""

[sources.file.options.exclude]
type = "[string]"
examples = [["/var/log/nginx/access.log"]]
null = false
description = """\
Array of file patterns to exclude. [Globbing](#globbing) is supported. \
*Takes precedence over the `include` option.*\
"""

[sources.file.options.file_key]
type = "string"
category = "Context"
default = "file"
null = false
section = "context"
description = """\
The key name added to each event with the full path of the file.\
"""

[sources.file.options.glob_minimum_cooldown]
type = "int"
default = 1000
null = false
unit = "milliseconds"
description = """\
Delay between file discovery calls. This controls the interval at which \
Vector searches for files.\
"""

[sources.file.options.host_key]
name = "host_key"
type = "string"
category = "Context"
default = "host"
null = false
section = "context"
description = """\
The key name added to each event representing the current host.\
"""

[sources.file.options.ignore_older]
type = "int"
examples = [86400]
null = true
unit = "seconds"
description = """\
Ignore files with a data modification date that does not exceed this age.\
"""

[sources.file.options.max_line_bytes]
type = "int"
unit = "bytes"
null = true
default = 102400
description = """\
The maximum number of a bytes a line can contain before being \
discarded. This protects against malformed lines or tailing incorrect \
files.\
"""

[sources.file.options.start_at_beginning]
type = "bool"
null = false
default = false
description = """\
When `true` Vector will read from the beginning of new files, when \
`false` Vector will only read new data added to the file.\
"""

[sources.file.options.fingerprinting]
type = "table"
null = true
description = """\
Configuration for how the file source should identify files.\
"""

[sources.file.options.fingerprinting.options.strategy]
type = "string"
enum = ["checksum", "device_and_inode"]
default = "checksum"
null = true
description = """\
Whether to use the content of a file to differentiate it (`checksum`) or the \
storage device and inode (`device_and_inode`). Depending on your log rotation \
strategy, one may be a better fit than the other.\
"""

[sources.file.options.fingerprinting.options.fingerprint_bytes]
type = "int"
default = 256
null = false
unit = "bytes"
relevant_when = {strategy = "checksum"}
description = """\
The number of bytes read off the head of the file to generate a unique \
fingerprint.\
"""

[sources.file.options.fingerprinting.options.ignored_header_bytes]
type = "int"
default = 0
null = false
unit = "bytes"
relevant_when = {strategy = "checksum"}
description = """\
The number of bytes to skip ahead (or ignore) when generating a unique \
fingerprint. This is helpful if all files share a common header.\
"""

[sources.file.options.message_start_indicator]
type = "string"
null = true
examples = ["^(INFO|ERROR)"]
description = """\
When present, Vector will aggregate multiple lines into a single event, using \
this pattern as the indicator that the previous lines should be flushed and \
a new event started. The pattern will be matched against entire lines as \
a regular expression, so remember to anchor as appropriate.\
"""

[sources.file.options.multi_line_timeout]
type = "int"
default = 1000
null = true
unit = "milliseconds"
description = """\
When `message_start_indicator` is present, this sets the amount of time Vector \
will buffer lines into a single event before flushing, regardless of whether \
or not it has seen a line indicating the start of a new message.\
"""

# ------------------------------------------------------------------------------
# sources.journald
# ------------------------------------------------------------------------------
[sources.journald]
beta = true
delivery_guarantee = "best_effort"
guides = []
output_types = ["log"]
resources = []
through_description = "log records from journald"

[sources.journald.options.current_runtime_only]
type = "bool"
null = true
default = true
description = "Include only entries from the current runtime (boot)"

[sources.journald.options.data_dir]
type = "string"
examples = ["/var/lib/vector"]
null = true
description = """\
The directory used to persist the journal checkpoint position. By \
default, the global `data_dir` is used. Please make sure the Vector \
project has write permissions to this dir. \
"""

[sources.journald.options.local_only]
type = "bool"
null = true
default = true
description = "Include only entries from the local system"

[sources.journald.options.units]
type = "[string]"
null = true
default = []
examples = [["ntpd", "sysinit.target"]]
description = """\
The list of units names to monitor. \
If empty or not present, all units are accepted. \
Unit names lacking a `"."` will have `".service"` appended to make them a valid service unit name.\
"""

# ------------------------------------------------------------------------------
# sources.kafka
# ------------------------------------------------------------------------------
[sources.kafka]
beta = true
delivery_guarantee = "best_effort"
guides = []
output_types = ["log"]
resources = []
through_description = "Kafka 0.9 or later"

[sources.kafka.options.bootstrap_servers]
type = "string"
examples = ["10.14.22.123:9092,10.14.23.332:9092"]
null = false
description = """\
A comma-separated list of host and port pairs that are the addresses of the \
Kafka brokers in a \"bootstrap\" Kafka cluster that a Kafka client connects \
to initially to bootstrap itself.\
"""

[sources.kafka.options.topics]
type = "[string]"
examples = [["topic-1", "topic-2", "topic-3"]]
null = false
description = """\
The Kafka topics names to read events from.
"""

[sources.kafka.options.group_id]
type = "string"
examples = ["consumer-group-name"]
null = false
description = """\
The consumer group name to be used to consume events from Kafka.
"""

[sources.kafka.options.key_field]
type = "string"
examples = ["user_id"]
null = true
description = """\
The field name to use for the topic key. If unspecified, the key would not \
be added to the events. If the message has null key, then this field would \
not be added to the event.\
"""

[sources.kafka.options.auto_offset_reset]
type = "string"
examples = ["smallest", "earliest", "beginning", "largest", "latest", "end", "error"]
null = true
default = "largest"
description = """\
If offsets for consumer group do not exist, set them using this strategy. \
[librdkafka documentation](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) \
for `auto.offset.reset` option for explanation.\
"""

[sources.kafka.options.session_timeout_ms]
type = "int"
examples = [5000, 10000]
null = true
default = 10000
unit = "milliseconds"
description = """\
The Kafka session timeout in milliseconds.
"""

# ------------------------------------------------------------------------------
# sources.statsd
# ------------------------------------------------------------------------------
[sources.statsd]
beta = true
delivery_guarantee = "best_effort"
guides = []
output_types = ["metric"]
resources = []
through_description = "the StatsD UDP protocol"

[sources.statsd.options.address]
type = "string"
null = false
examples = ["127.0.0.1:8126"]
description = "UDP socket address to bind to."

# ------------------------------------------------------------------------------
# sources.stdin
# ------------------------------------------------------------------------------
[sources.stdin]
delivery_guarantee = "at_least_once"
guides = []
output_types = ["log"]
resources = []
through_description = "standard input (STDIN)"

[sources.stdin.options.max_length]
type = "int"
default = 102400
null = true
unit = "bytes"
description = "The maxiumum bytes size of a message before it is discarded."

[sources.stdin.options.host_key]
type = "string"
category = "Context"
null = false
default = "host"
section = "context"
description = """\
The key name added to each event representing the current host.\
"""

# ------------------------------------------------------------------------------
# sources.syslog
# ------------------------------------------------------------------------------
[sources.syslog]
delivery_guarantee = "best_effort"
guides = []
output_types = ["log"]
resources = []
through_description = "the Syslog 5424 protocol"

[sources.syslog.options.address]
type = "string"
examples = ["0.0.0.0:9000"]
null = true
description = "The TCP or UDP address to listen on."

[sources.syslog.options.host_key]
name = "host_key"
type = "string"
category = "Context"
null = false
default = "host"
section = "context"
description = """\
The key name added to each event representing the current host.\
"""

[sources.syslog.options.max_length]
type = "int"
default = 102400
null = true
unit = "bytes"
description = """\
The maximum bytes size of incoming messages before they are discarded.\
"""

[sources.syslog.options.mode]
type = "string"
enum = ["tcp", "udp", "unix"]
null = false
description = "The input mode."

[sources.syslog.options.path]
type = "string"
examples = ["/path/to/socket"]
null = true
relevant_when = {mode = "unix"}
description = """\
The unix socket path. *This should be absolute path.*
"""

# ------------------------------------------------------------------------------
# sources.tcp
# ------------------------------------------------------------------------------
[sources.tcp]
delivery_guarantee = "best_effort"
guides = []
output_types = ["log"]
resources = []
through_description = "the TCP protocol"

[sources.tcp.options.address]
type = "string"
examples = ["0.0.0.0:9000"]
null = false
description = "The address to bind the socket to."

[sources.tcp.options.host_key]
name = "host_key"
type = "string"
category = "Context"
null = false
default = "host"
section = "context"
description = """\
The key name added to each event representing the current host.\
"""

[sources.tcp.options.max_length]
type = "int"
default = 102400
null = true
unit = "bytes"
description = """\
The maximum bytes size of incoming messages before they are discarded.\
"""

[sources.tcp.options.shutdown_timeout_secs]
type = "int"
default = 30
null = false
unit = "seconds"
description = """\
The timeout before a connection is forcefully closed during shutdown.\
"""

# ------------------------------------------------------------------------------
# sources.udp
# ------------------------------------------------------------------------------
[sources.udp]
delivery_guarantee = "best_effort"
guides = []
output_types = ["log"]
resources = []
through_description = "the UDP protocol"

[sources.udp.options.address]
type = "string"
examples = ["0.0.0.0:9000"]
null = false
description = "The address to bind the socket to."

[sources.udp.options.host_key]
name = "host_key"
type = "string"
category = "Context"
null = false
default = "host"
section = "context"
description = """\
The key name added to each event representing the current host.\
"""

[sources.udp.options.max_length]
type = "int"
default = 102400
null = true
unit = "bytes"
description = """\
The maximum bytes size of incoming messages before they are discarded.\
"""

# ------------------------------------------------------------------------------
# sources.vector
# ------------------------------------------------------------------------------
[sources.vector]
beta = true
delivery_guarantee = "best_effort"
guides = []
output_types = ["log", "metric"]
resources = []
through_description = "another upstream Vector instance"

[sources.vector.options.address]
type = "string"
examples = ["0.0.0.0:9000"]
null = false
description = "The TCP address to bind to."

[sources.vector.options.shutdown_timeout_secs]
type = "int"
default = 30
null = false
unit = "seconds"
description = """\
The timeout before a connection is forcefully closed during shutdown.\
"""

# ------------------------------------------------------------------------------
# transforms.add_fields
# ------------------------------------------------------------------------------
[transforms.add_fields]
allow_you_to_description = "add one or more fields"
function_categories = ["change_fields"]
guides = []
input_types = ["log"]
output_types = ["log"]
resources = []

[transforms.add_fields.options.fields]
type = "table"
null = false
description = """\
A table of key/value pairs representing the keys to be added to the \
event.\
"""

[transforms.add_fields.options.fields.options."*"]
type = "*"
null = false
examples = [
  {name = "my_string_field", value = "string value"},
  {name = "my_env_var_field", value = "${ENV_VAR}"},
  {name = "my_int_field", value = 1},
  {name = "my_float_field", value = 1.2},
  {name = "my_bool_field", value = true},
  {name = "my_timestamp_field", value = 1979-05-27T00:32:00.999999-07:00},
  {name = "my_nested_fields", value = {key1 = "value1", key2 = "value2"}},
  {name = "my_list", value = ["first", "second", "third"]},
]
description = """\
A key/value pair representing the new field to be added. Accepts all \
[supported types][docs.config_value_types]. Use `.` for adding nested fields.\
"""

# ------------------------------------------------------------------------------
# transforms.coercer
# ------------------------------------------------------------------------------
[transforms.coercer]
allow_you_to_description = "coerce event fields into fixed types"
function_categories = ["coerce"]
guides = []
input_types = ["log"]
output_types = ["log"]
resources = []

[transforms.coercer.options.types]
type = "table"
null = true
description = "Key/Value pairs representing mapped field types."

[transforms.coercer.options.types.options."*"]
type = "string"
enum = ["string", "int", "float", "bool", "timestamp|strftime"]
examples = [
  {name = "status", value = "int"},
  {name = "duration", value = "float"},
  {name = "success", value = "bool"},
  {name = "timestamp", value = "timestamp|%s", comment = "unix"},
  {name = "timestamp", value = "timestamp|%+", comment = "iso8601 (date and time)"},
  {name = "timestamp", value = "timestamp|%F", comment = "iso8601 (date)"},
  {name = "timestamp", value = "timestamp|%a %b %e %T %Y", comment = "custom strftime format"},
]
null = false
description = """\
A definition of field type conversions. They key is the field name and the value \
is the type. [`strftime` specifiers][url.strftime_specifiers] are supported for the `timestamp` type.\
"""

# ------------------------------------------------------------------------------
# transforms.field_filter
# ------------------------------------------------------------------------------
[transforms.field_filter]
allow_you_to_description = "filter events by a field's value"
beta = true
function_categories = ["filter"]
guides = []
input_types = ["log", "metric"]
output_types = ["log", "metric"]
resources = []

[transforms.field_filter.options.field]
type = "string"
examples = ["file"]
null = false
description = "The target field to compare against the `value`."

[transforms.field_filter.options.value]
type = "string"
examples = ["/var/log/nginx.log"]
null = false
description = """\
If the value of the specified `field` matches this value then the event \
will be permitted, otherwise it is dropped.\
"""

# ------------------------------------------------------------------------------
# transforms.grok_parser
# ------------------------------------------------------------------------------
[transforms.grok_parser]
allow_you_to_description = "parse a field value with [Grok][url.grok]"
function_categories = ["parse"]
guides = []
input_types = ["log"]
output_types = ["log"]
resources = [
  {name = "Grok Debugger", short_link = "grok_debugger"},
  {name = "Grok Patterns", short_link = "grok_patterns"}
]

[transforms.grok_parser.options.drop_field]
type = "bool"
default = true
null = false
description = """\
If `true` will drop the `field` after parsing.\
"""

[transforms.grok_parser.options.field]
type = "string"
default = "message"
null = false
description = """\
The field to execute the `pattern` against. Must be a `string` value.\
"""

[transforms.grok_parser.options.pattern]
type = "string"
examples = ["%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}"]
null = false
description = "The [Grok pattern][url.grok_patterns]"

[transforms.grok_parser.options.types]
type = "table"
null = true
description = "Key/Value pairs representing mapped field types."

[transforms.grok_parser.options.types.options."*"]
type = "string"
enum = ["string", "int", "float", "bool", "timestamp|strftime"]
examples = [
  {name = "status", value = "int"},
  {name = "duration", value = "float"},
  {name = "success", value = "bool"},
  {name = "timestamp", value = "timestamp|%s", comment = "unix"},
  {name = "timestamp", value = "timestamp|%+", comment = "iso8601 (date and time)"},
  {name = "timestamp", value = "timestamp|%F", comment = "iso8601 (date)"},
  {name = "timestamp", value = "timestamp|%a %b %e %T %Y", comment = "custom strftime format"},
]
null = false
description = """\
A definition of mapped field types. They key is the field name and the value \
is the type. [`strftime` specifiers][url.strftime_specifiers] are supported for the `timestamp` type.\
"""

# ------------------------------------------------------------------------------
# transforms.json_parser
# ------------------------------------------------------------------------------
[transforms.json_parser]
allow_you_to_description = "parse a field value as JSON"
function_categories = ["parse_json"]
guides = []
input_types = ["log"]
output_types = ["log"]
resources = []

[transforms.json_parser.options.drop_invalid]
type = "bool"
examples = [true]
null = false
description = """\
If `true` events with invalid JSON will be dropped, otherwise the \
event will be kept and passed through.\
"""

[transforms.json_parser.options.field]
type = "string"
default = "message"
null = false
description = """\
The field decode as JSON. Must be a `string` value.\
"""

# ------------------------------------------------------------------------------
# transforms.log_to_metric
# ------------------------------------------------------------------------------
[transforms.log_to_metric]
allow_you_to_description = "convert logs into one or more metrics"
function_categories = ["convert_types"]
guides = []
input_types = ["log"]
output_types = ["metric"]
resources = []

[transforms.log_to_metric.options.metrics]
type = "[table]"
null = false
description = """\
A table of key/value pairs representing the keys to be added to the \
event.\
"""

[transforms.log_to_metric.options.metrics.options.type]
type = "string"
enum = ["counter", "gauge", "histogram", "set"]
null = false
description = "The metric type."

[transforms.log_to_metric.options.metrics.options.field]
type = "string"
examples = ["duration"]
null = false
description = "The log field to use as the metric."

[transforms.log_to_metric.options.metrics.options.increment_by_value]
type = "bool"
default = false
null = false
relevant_when = {type = "counter"}
description = """\
If `true` the metric will be incremented by the `field` value. If `false` \
the metric will be incremented by 1 regardless of the `field` value.\
"""

[transforms.log_to_metric.options.metrics.options.name]
type = "string"
examples = ["duration_total"]
null = false
description = """\
The name of the metric. Defaults to `<field>_total` for `counter` and \
`<field>` for `gauge`.\
"""

[transforms.log_to_metric.options.metrics.options.tags]
type = "table"
display = "inline"
null = true
description = "Key/value pairs representing metric tags."

[transforms.log_to_metric.options.metrics.options.tags.options."*"]
type = "string"
examples = [
  {name = "host", value = "${HOSTNAME}"},
  {name = "region", value = "us-east-1"},
  {name = "status", value = "{{status}}"},
]
null = false
description = "Key/value pairs representing the metric tags."

# ------------------------------------------------------------------------------
# transforms.lua
# ------------------------------------------------------------------------------
[transforms.lua]
allow_you_to_description = "transform events with a full embedded [Lua][url.lua] engine"
beta = true
function_categories = ["program"]
guides = []
input_types = ["log"]
output_types = ["log"]
resources = [
  {name = "Lua Reference Manual", short_link = "lua_manual"}
]

[transforms.lua.options.source]
type = "string"
examples = [
"""\
require("script") # a `script.lua` file must be in your `search_dirs`

if event["host"] == nil then
  local f = io.popen ("/bin/hostname")
  local hostname = f:read("*a") or ""
  f:close()
  hostname = string.gsub(hostname, "\\n$", "")
  event["host"] = hostname
end\
"""
]
null = false
description = "The inline Lua source to evaluate."

[transforms.lua.options.search_dirs]
type = "[string]"
examples = [["/etc/vector/lua"]]
null = true
description = """\
A list of directories search when loading a Lua file via the `require` \
function.\
"""

# ------------------------------------------------------------------------------
# transforms.regex_parser
# ------------------------------------------------------------------------------
[transforms.regex_parser]
allow_you_to_description = """\
parse a field's value with a [Regular Expression][url.regex]\
"""
function_categories = ["parse"]
guides = []
input_types = ["log"]
output_types = ["log"]
resources = []

[transforms.regex_parser.options.drop_field]
type = "bool"
default = true
null = false
description = "If the `field` should be dropped (removed) after parsing."

[transforms.regex_parser.options.field]
type = "string"
default = "message"
null = false
description = "The field to parse."

[transforms.regex_parser.options.regex]
type = "string"
examples = [
"""\
^(?P<host>[\\w\\.]+) - (?P<user>[\\w]+) (?P<bytes_in>[\\d]+) \\[(?P<timestamp>.*)\\] "(?P<method>[\\w]+) (?P<path>.*)" (?P<status>[\\d]+) (?P<bytes_out>[\\d]+)$\
"""
]
null = false
description = """\
The Regular Expression to apply. Do not inlcude the leading or trailing `/`.\
"""

[transforms.regex_parser.options.types]
type = "table"
null = true
description = "Key/Value pairs representing mapped field types."

[transforms.regex_parser.options.types.options."*"]
type = "string"
enum = ["string", "int", "float", "bool", "timestamp|strftime"]
examples = [
  {name = "status", value = "int"},
  {name = "duration", value = "float"},
  {name = "success", value = "bool"},
  {name = "timestamp", value = "timestamp|%s", comment = "unix"},
  {name = "timestamp", value = "timestamp|%+", comment = "iso8601 (date and time)"},
  {name = "timestamp", value = "timestamp|%F", comment = "iso8601 (date)"},
  {name = "timestamp", value = "timestamp|%a %b %e %T %Y", comment = "custom strftime format"},
]
null = false
description = """\
A definition of mapped field types. They key is the field name and the value \
is the type. [`strftime` specifiers][url.strftime_specifiers] are supported for the `timestamp` type.\
"""

[[transforms.regex_parser.resources]]
name = "Regex Tester"
short_link = "regex_tester"

[[transforms.regex_parser.resources]]
name = "Rust Regex Syntax"
short_link = "rust_regex_syntax"

# ------------------------------------------------------------------------------
# transforms.remove_fields
# ------------------------------------------------------------------------------
[transforms.remove_fields]
allow_you_to_description = "remove one or more event fields"
function_categories = ["change_fields"]
input_types = ["log", "metric"]
output_types = ["log", "metric"]

[transforms.remove_fields.options.fields]
type = "[string]"
examples = [["field1", "field2"]]
null = false
description = "The field names to drop."

# ------------------------------------------------------------------------------
# transforms.sampler
# ------------------------------------------------------------------------------
[transforms.sampler]
allow_you_to_description = "sample events with a configurable rate"
beta = true
function_categories = ["sample"]
input_types = ["log"]
output_types = ["log"]

[transforms.sampler.options.pass_list]
type = "[string]"
examples = [["[error]", "field2"]]
null = true
description = """\
A list of regular expression patterns to exclude events from sampling. \
If an event's `"message"` key matches _any_ of these patterns it will \
_not_ be sampled.\
"""

[transforms.sampler.options.rate]
type = "int"
examples = [10]
null = false
description = "The maximum number of events allowed per second."

## TODO: Add regex synax docs?

# ------------------------------------------------------------------------------
# transforms.tokenizer
# ------------------------------------------------------------------------------
[transforms.tokenizer]
allow_you_to_description = """\
tokenize a field's value by splitting on white space, ignoring special \
wrapping characters, and zipping the tokens into ordered field names\
"""
function_categories = ["parse"]
input_types = ["log"]
output_types = ["log"]

[transforms.tokenizer.options.field]
type = "string"
default = "message"
null = false
description = "The field to tokenize."

[transforms.tokenizer.options.field_names]
type = "[string]"
examples = [["timestamp", "level", "message"]]
null = false
description = "The field names assigned to the resulting tokens, in order."

[transforms.tokenizer.options.drop_field]
type = "bool"
default = true
null = false
description = "If `true` the `field` will be dropped after parsing."

[transforms.tokenizer.options.types]
type = "table"
null = true
description = "Key/Value pairs representing mapped field types."

[transforms.tokenizer.options.types.options."*"]
type = "string"
enum = ["string", "int", "float", "bool", "timestamp|strftime"]
examples = [
  {name = "status", value = "int"},
  {name = "duration", value = "float"},
  {name = "success", value = "bool"},
  {name = "timestamp", value = "timestamp|%s", comment = "unix"},
  {name = "timestamp", value = "timestamp|%+", comment = "iso8601 (date and time)"},
  {name = "timestamp", value = "timestamp|%F", comment = "iso8601 (date)"},
  {name = "timestamp", value = "timestamp|%a %b %e %T %Y", comment = "custom strftime format"},
]
null = false
description = """\
A definition of mapped field types. They key is the field name and the value \
is the type. [`strftime` specifiers][url.strftime_specifiers] are supported for the `timestamp` type.\
"""

# ------------------------------------------------------------------------------
# sinks.aws_cloudwatch_logs
# ------------------------------------------------------------------------------
[sinks.aws_cloudwatch_logs]
batch_size = 1049000
batch_timeout = 1
beta = true
buffer = true
delivery_guarantee = "at_least_once"
egress_method = "batching"
healthcheck = true
input_types = ["log"]
rate_limit_duration = 1
rate_limit_num = 5
retry_attempts = 5
retry_backoff_secs = 1
request_in_flight_limit = 5
request_timeout_secs = 30
service_limits_short_link = "aws_cw_logs_service_limits"
service_provider = "AWS"
write_to_description = "[AWS CloudWatch Logs][url.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html)"

[sinks.aws_cloudwatch_logs.options.encoding]
type = "string"
category = "Requests"
enum = ["json", "text"]
null = true
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.aws_cloudwatch_logs.options.group_name]
type = "string"
examples = [
  "{{ file }}",
  "ec2/{{ instance_id }}",
  "group-name"
]
null = false
partition_key = true
templateable = true
description = "The [group name][url.aws_cw_logs_group_name] of the target CloudWatch Logs stream."

[sinks.aws_cloudwatch_logs.options.region]
type = "string"
examples = ["us-east-1"]
null = false
description = "The [AWS region][url.aws_cw_logs_regions] of the target CloudWatch Logs stream resides."

[sinks.aws_cloudwatch_logs.options.stream_name]
type = "string"
examples = [
  "{{ instance_id }}",
  "%Y-%m-%d",
  "stream-name"
]
null = false
partition_key = true
templateable = true
description = "The [stream name][url.aws_cw_logs_stream_name] of the target CloudWatch Logs stream."

[sinks.aws_cloudwatch_logs.options.create_missing_group]
type = "bool"
default = true
null = true
description = """\
Dynamically create a [log group][url.aws_cw_logs_group_name] if it does not already exist. This will ignore \
`create_missing_stream` directly after creating the group and will create the first stream. \
"""

[sinks.aws_cloudwatch_logs.options.create_missing_stream]
type = "bool"
default = true
null = true
description = "Dynamically create a [log stream][url.aws_cw_logs_stream_name] if it does not already exist."

# ------------------------------------------------------------------------------
# sinks.aws_kinesis_streams
# ------------------------------------------------------------------------------
[sinks.aws_kinesis_streams]
batch_size = 1049000
batch_timeout = 1
beta = true
buffer = true
delivery_guarantee = "at_least_once"
egress_method = "batching"
healthcheck = true
input_types = ["log"]
rate_limit_duration = 1
rate_limit_num = 5
retry_attempts = 5
retry_backoff_secs = 1
request_in_flight_limit = 5
request_timeout_secs = 30
service_limits_short_link = "aws_kinesis_service_limits"
service_provider = "AWS"
write_to_description = "[AWS Kinesis Data Stream][url.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)"

[sinks.aws_kinesis_streams.options.encoding]
type = "string"
category = "Requests"
enum = ["json", "text"]
null = true
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.aws_kinesis_streams.options.partition_key_field]
type = "string"
examples = ["user_id"]
null = true
description = "The event field used as the Kinesis record's partition key value."

[sinks.aws_kinesis_streams.options.region]
type = "string"
examples = ["us-east-1"]
null = false
description = "The [AWS region][url.aws_cw_logs_regions] of the target Kinesis stream resides."

[sinks.aws_kinesis_streams.options.stream_name]
type = "string"
examples = ["my-stream"]
null = false
description = "The [stream name][url.aws_cw_logs_stream_name] of the target Kinesis Logs stream."

# ------------------------------------------------------------------------------
# sinks.aws_s3
# ------------------------------------------------------------------------------
[sinks.aws_s3]
batch_size = 10490000
batch_timeout = 300
beta = true
buffer = true
delivery_guarantee = "at_least_once"
egress_method = "batching"
healthcheck = true
input_types = ["log"]
rate_limit_duration = 1
rate_limit_num = 5
retry_attempts = 5
retry_backoff_secs = 1
request_in_flight_limit = 5
request_timeout_secs = 30
service_limits_short_link = "aws_s3_service_limits"
service_provider = "AWS"
write_to_description = "[AWS S3][url.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)"

[sinks.aws_s3.options.bucket]
type = "string"
examples = ["my-bucket"]
null = false
description = "The S3 bucket name. Do not include a leading `s3://` or a trailing `/`."

[sinks.aws_s3.options.compression]
type = "string"
category = "Requests"
enum = ["gzip"]
null = true
description = "The compression type to use before writing data."

[sinks.aws_s3.options.encoding]
type = "string"
category = "Requests"
enum = ["ndjson", "text"]
null = true
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.aws_s3.options.filename_append_uuid]
type = "bool"
category = "Object Names"
default = true
null = false
description = "Whether or not to append a UUID v4 token to the end of the file. This ensures there are no name collisions high volume use cases."

[sinks.aws_s3.options.filename_extension]
type = "bool"
category = "Object Names"
default = "log"
null = false
description = "The extension to use in the object name."

[sinks.aws_s3.options.filename_time_format]
type = "string"
category = "Object Names"
default = "%s"
null = false
description = "The format of the resulting object file name. [`strftime` specifiers][url.strftime_specifiers] are supported."

[sinks.aws_s3.options.gzip]
type = "bool"
category = "Requests"
default = false
null = false
description = "Whether to Gzip the content before writing or not. Please note, enabling this has a slight performance cost but significantly reduces bandwidth."

[sinks.aws_s3.options.key_prefix]
type = "string"
category = "Object Names"
default = "date=%F"
examples = [
  "date=%F/",
  "date=%F/hour=%H/",
  "year=%Y/month=%m/day=%d/",
  "application_id={{ application_id }}/date=%F/"
]
null = true
partition_key = true
templateable = true
description = "A prefix to apply to all object key names. This should be used to partition your objects, and it's important to end this value with a `/` if you want this to be the root S3 \"folder\"."

[sinks.aws_s3.options.region]
type = "string"
examples = ["us-east-1"]
null = false
description = "The [AWS region][url.aws_s3_regions] of the target S3 bucket."

# ------------------------------------------------------------------------------
# sinks.blachole
# ------------------------------------------------------------------------------
[sinks.blackhole]
buffer = false
delivery_guarantee = "best_effort"
egress_method = "streaming"
healthcheck = true
input_types = ["log", "metric"]
write_to_description = """\
a blackhole that simply discards data, designed for testing and \
benchmarking purposes\
"""

[sinks.blackhole.options.print_amount]
type = "int"
examples = [1000]
null = false
description = "The number of events that must be received in order to print a summary of activity."

# ------------------------------------------------------------------------------
# sinks.console
# ------------------------------------------------------------------------------

[sinks.clickhouse]
batch_size = 1049000
batch_timeout = 1
beta = true
buffer = false
delivery_guarantee = "best_effort"
healthcheck = true
egress_method = "batching"
input_types = ["log"]
rate_limit_duration = 1
rate_limit_num = 5
retry_attempts = 9223372036854775807
retry_backoff_secs = 1
request_in_flight_limit = 5
request_timeout_secs = 30
write_to_description = "[Clickhouse][url.clickhouse] via the [`HTTP` Interface][url.clickhouse_http]"

[sinks.clickhouse.options.compression]
type = "string"
category = "Requests"
enum = ["gzip"]
null = true
description = "The compression type to use before writing data."

[sinks.clickhouse.options.host]
type = "string"
examples = ["http://localhost:8123"]
null = false
description = "The host url of the [Clickhouse][url.clickhouse] server."

[sinks.clickhouse.options.table]
type = "string"
examples = ["mytable"]
null = false
description = "The table that data will be inserted into."

[sinks.clickhouse.options.database]
type = "string"
examples = ["mydatabase"]
null = true
description = "The database that contains the stable that data will be inserted into."

# ------------------------------------------------------------------------------
# sinks.console
# ------------------------------------------------------------------------------
[sinks.console]
buffer = false
delivery_guarantee = "best_effort"
egress_method = "streaming"
healthcheck = true
input_types = ["log", "metric"]
write_to_description = "the console, `STDOUT` or `STDERR`"

[sinks.console.options.encoding]
type = "string"
enum = ["json", "text"]
null = true
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.console.options.target]
type = "string"
defaukt = "stdout"
enum = ["stdout", "stderr"]
null = false
description = "The [standard stream][url.standard_streams] to write to."

# ------------------------------------------------------------------------------
# sinks.elasticsearch
# ------------------------------------------------------------------------------
[sinks.elasticsearch]
batch_size = 10490000
batch_timeout = 1
beta = true
buffer = true
delivery_guarantee = "best_effort"
egress_method = "batching"
healthcheck = true
input_types = ["log"]
rate_limit_duration = 1
rate_limit_num = 5
retry_attempts = 5
retry_backoff_secs = 1
request_in_flight_limit = 5
request_timeout_secs = 60
service_provider = "Elastic"
write_to_description = "[Elasticsearch][url.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html)"

[sinks.elasticsearch.options.doc_type]
type = "string"
default = "_doc"
null = false
description = "The `doc_type` for your index data. This is only relevant for Elasticsearch <= 6.X. If you are using >= 7.0 you do not need to set this option since Elasticsearch has removed it."

[sinks.elasticsearch.options.host]
type = "string"
examples = ["http://10.24.32.122:9000"]
null = false
description = "The host of your Elasticsearch cluster. This should be the full URL as shown in the example."

[sinks.elasticsearch.options.index]
type = "string"
default = "vector-%F"
examples = [
  "vector-%Y-%m-%d",
  "application-{{ application_id }}-%Y-%m-%d"
]
null = false
templateable = true
description = "Index name to write events to."

[sinks.elasticsearch.options.basic_auth]
type = "table"
null = true
description = "Options for basic authentication."

[sinks.elasticsearch.options.basic_auth.options.password]
type = "string"
examples = ["password"]
null = false
description = "The basic authentication password."

[sinks.elasticsearch.options.basic_auth.options.user]
type = "string"
examples = ["username"]
null = false
description = "The basic authentication user name."

[sinks.elasticsearch.options.headers]
type = "table"
null = true
description = "Options for custom headers."

[sinks.elasticsearch.options.headers.options."*"]
type = "string"
examples = [{ name = "X-Powered-By", value = "Vector"}]
null = false
description = "A custom header to be added to each outgoing Elasticsearch request."

[sinks.elasticsearch.options.provider]
type = "string"
enum = ["default", "aws"]
null = true
default = "default"
description = "The provider of the Elasticsearch service."

[sinks.elasticsearch.options.query]
type = "table"
null = true
description = "Custom parameters to Elasticsearch query string."

[sinks.elasticsearch.options.query.options."*"]
type = "string"
examples = [{ name = "X-Powered-By", value = "Vector"}]
null = false
description = "A custom parameter to be added to each Elasticsearch request."

[sinks.elasticsearch.options.region]
type = "string"
examples = ["us-east-1"]
null = true
description = "When using the AWS provider, the [AWS region][url.aws_cw_logs_regions] of the target Elasticsearch instance."

# ------------------------------------------------------------------------------
# sinks.http
# ------------------------------------------------------------------------------
[sinks.http]
batch_size = 1049000
batch_timeout = 5
buffer = true
delivery_guarantee = "at_least_once"
egress_method = "batching"
healthcheck = true
input_types = ["log"]
rate_limit_duration = 1
rate_limit_num = 10
retry_attempts = 10
retry_backoff_secs = 1
request_in_flight_limit = 10
request_timeout_secs = 30
service_provider = "Elastic"
write_to_description = "a generic HTTP endpoint"

[sinks.http.options.basic_auth]
type = "table"
null = true
description = "Options for basic authentication."

[sinks.http.options.basic_auth.options.password]
type = "string"
examples = ["password"]
null = false
description = "The basic authentication password."

[sinks.http.options.basic_auth.options.user]
type = "string"
examples = ["username"]
null = false
description = "The basic authentication user name."

[sinks.http.options.compression]
type = "string"
enum = ["gzip"]
null = true
description = "The compression strategy used to compress the payload before sending."

[sinks.http.options.encoding]
type = "string"
enum = ["ndjson", "text"]
null = false
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.http.options.headers]
type = "table"
null = true
description = "Options for custom headers."

[sinks.http.options.headers.options."*"]
type = "string"
examples = [{ name = "X-Powered-By", value = "Vector"}]
null = false
description = "A custom header to be added to each outgoing HTTP request."

[sinks.http.options.healthcheck_uri]
type = "string"
examples = ["https://10.22.212.22:9000/_health"]
null = true
description = "A URI that Vector can request in order to determine the service health."

[sinks.http.options.uri]
type = "string"
examples = ["https://10.22.212.22:9000/endpoint"]
null = false
description = "The full URI to make HTTP requests to. This should include the protocol and host, but can also include the port, path, and any other valid part of a URI."

[sinks.http.options.verify_certificate]
type = "bool"
null = true
default = true
description = """When making a connection to a HTTPS server, this \
controls if the TLS certificate presented by the server will be \
verified. Do not set this unless you know what you are doing. Turning \
this off introduces significant vulnerabilities."""

# ------------------------------------------------------------------------------
# sinks.kafka
# ------------------------------------------------------------------------------
[sinks.kafka]
buffer = true
delivery_guarantee = "at_least_once"
egress_method = "streaming"
healthcheck = true
input_types = ["log"]
service_provider = "Confluent"
write_to_description = "[Apache Kafka][url.kafka] via the [Kafka protocol][url.kafka_protocol]"

[sinks.kafka.options.bootstrap_servers]
type = "string"
examples = ["10.14.22.123:9092,10.14.23.332:9092"]
null = false
description = """\
A comma-separated list of host and port pairs that are the addresses of the \
Kafka brokers in a \"bootstrap\" Kafka cluster that a Kafka client connects \
to initially to bootstrap itself\
"""

[sinks.kafka.options.encoding]
type = "string"
enum = ["json", "text"]
null = true
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.kafka.options.key_field]
type = "string"
examples = ["user_id"]
null = false
description = """\
The field name to use for the topic key. If unspecified, the key will be \
randomly generated. If the field does not exist on the event, a blank value \
will be used.\
"""

[sinks.kafka.options.topic]
type = "string"
examples = ["topic-1234"]
null = false
description = "The Kafka topic name to write events to."

# ------------------------------------------------------------------------------
# sinks.prometheus
# ------------------------------------------------------------------------------
[sinks.prometheus]
beta = true
buffer = false
delivery_guarantee = "best_effort"
egress_method = "exposing"
healthcheck = false
input_types = ["metric"]
write_to_description = "[Prometheus][url.prometheus] metrics service"

[sinks.prometheus.options.address]
type = "string"
examples = ["0.0.0.0:9598"]
null = false
description = "The address to expose for scraping."

[sinks.prometheus.options.namespace]
type = "string"
examples = ["service"]
null = false
description = """\
A prefix that will be added to all metric names.
It should follow Prometheus [naming conventions][url.prometheus_metric_naming].\
"""

[sinks.prometheus.options.buckets]
type = "[float]"
default = [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
null = false
unit = "seconds"
description = """\
Default buckets to use for [histogram][docs.metric_event.histogram] metrics.\
"""

# ------------------------------------------------------------------------------
# sinks.splunk_hec
# ------------------------------------------------------------------------------
[sinks.splunk_hec]
batch_size = 1049000
batch_timeout = 1
buffer = true
delivery_guarantee = "at_least_once"
egress_method = "batching"
healthcheck = true
input_types = ["log"]
rate_limit_duration = 1
rate_limit_num = 10
retry_attempts = 5
retry_backoff_secs = 1
request_in_flight_limit = 10
request_timeout_secs = 60
service_provider = "Splunk"
write_to_description = "a [Splunk HTTP Event Collector][url.splunk_hec]"

[sinks.splunk_hec.options.encoding]
type = "string"
category = "Requests"
enum = ["ndjson", "text"]
null = true
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.splunk_hec.options.host]
type = "string"
examples = ["my-splunk-host.com"]
null = false
description = "Your Splunk HEC host."

[sinks.splunk_hec.options.token]
type = "string"
examples = ["A94A8FE5CCB19BA61C4C08"]
null = false
description = "Your Splunk HEC token."

# ------------------------------------------------------------------------------
# sinks.tcp
# ------------------------------------------------------------------------------
[sinks.tcp]
buffer = true
delivery_guarantee = "best_effort"
egress_method = "streaming"
healthcheck = true
input_types = ["log"]
write_to_description = "a TCP connection"

[sinks.tcp.options.address]
type = "string"
examples = ["92.12.333.224:5000"]
null = false
description = "The TCP address."

[sinks.tcp.options.encoding]
type = "string"
category = "Requests"
enum = ["json", "text"]
null = true
description = """\
The encoding format used to serialize the events before flushing. The default \
is dynamic based on if the event is structured or not.\
"""

[sinks.tcp.options.tls]
type = "table"
null = true
description = "Options for TLS support"

[sinks.tcp.options.tls.options.enabled]
type = "bool"
null = true
default = false
description = "Enable TLS during connections to the remote."

[sinks.tcp.options.tls.options.verify]
type = "bool"
null = true
default = true
description = """If `true`, Vector will force certificate validation. \
Do NOT set this to `false` unless you know the risks of not verifying \
the remote certificate."""

[sinks.tcp.options.tls.options.ca_file]
type = "string"
null = true
description = "Absolute path to additional CA certificate file, in PEM format."
examples = ["/path/to/certificate_authority.crt"]

[sinks.tcp.options.tls.options.crt_file]
type = "string"
null = true
description = """Absolute path to certificate file used to identify this \
connection, in PEM format. If this is set, `key_file` must also be set."""
examples = ["/path/to/host_certificate.crt"]

[sinks.tcp.options.tls.options.key_file]
type = "string"
null = true
description = """Absolute path to key file used to identify this \
connection, in PEM format. If this is set, `crt_file` must also be set."""
examples = ["/path/to/host_certificate.key"]

[sinks.tcp.options.tls.options.key_phrase]
type = "string"
null = true
description = """Pass phrase to unlock the encrypted key file. \
This has no effect unless `key_file` above is set."""
examples = ["PassWord1"]

# ------------------------------------------------------------------------------
# sinks.vector
# ------------------------------------------------------------------------------
[sinks.vector]
buffer = true
delivery_guarantee = "best_effort"
egress_method = "streaming"
healthcheck = true
input_types = ["log"]
write_to_description = "another downstream Vector instance"

[sinks.vector.options.address]
type = "string"
examples = ["92.12.333.224:5000"]
null = false
description = "The downstream Vector address."

# ------------------------------------------------------------------------------
# enums
# ------------------------------------------------------------------------------
[enums]
correctness_tests = [
  "file_rotate_create_correctness",
  "file_rotate_truncate_correctness",
  "file_truncate_correctness",
  "wrapped_json_correctness"
]

performance_tests = [
  "file_to_tcp_performance",
  "regex_parsing_performance",
  "tcp_to_blackhole_performance",
  "tcp_to_http_performance",
  "tcp_to_tcp_performance"
]

delivery_guarantees = ["at_least_once", "best_effort"]
event_types = ["log", "metric"]

# ------------------------------------------------------------------------------
# links
# ------------------------------------------------------------------------------
[links]

[links.docs]
agent_role = "/setup/deployment/roles/agent.md"
at_least_once_delivery = "/about/guarantees.md#at-least-once-delivery"
best_effort_delivery = "/about/guarantees.md#best-effort-delivery"
config_composition = "/usage/configuration/README.md#composition"
config_value_types = "/usage/configuration/README.md#value-types"
data_directory = "/usage/configuration/README.md#data-directory"
default_schema = "/about/data-model/log.md#default-schema"
download = "/setup/installation/manual/from-archives#1-download-the-archive"
event = "/about/data-model/README.md#event"
event_key_special_characters = "/about/data-model/log.md#special-characters"
from_source = "/setup/installation/manual/from-source.md"
log_event = "/about/data-model/log.md"
metric_event = "/about/data-model/metric.md"
metrics = "/usage/administration/monitoring.md#metrics"
monitoring_logs = "/usage/administration/monitoring.md#logs"
pipelines = "/usage/configuration/README.md#composition"
service_role = "/setup/deployment/roles/service.md"

[links.url]
add_company = "https://github.com/timberio/vector/blob/master/.companies.toml"
apt = "https://wiki.debian.org/Apt"
aws_athena = "https://aws.amazon.com/athena/"
aws_athena_console = "https://console.aws.amazon.com/athena/home"
aws_access_keys = "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html"
aws_cloudwatch_logs_sink_source = "https://github.com/timberio/vector/blob/master/src/sinks/aws_cloudwatch_logs/mod.rs"
aws_elb = "https://aws.amazon.com/elasticloadbalancing/"
aws_credentials_file = "https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html"
aws_credential_process = "https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sourcing-external.html"
aws_cw_logs = "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"
aws_cw_logs_group_name = "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html"
aws_cw_logs_regions = "https://docs.aws.amazon.com/general/latest/gr/rande.html#cw_region"
aws_cw_logs_service_limits = "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch_limits_cwl.html"
aws_cw_logs_stream_name = "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html"
aws_kinesis_data_streams = "https://aws.amazon.com/kinesis/data-streams/"
aws_kinesis_partition_key = "https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecordsRequestEntry.html#Streams-Type-PutRecordsRequestEntry-PartitionKey"
aws_kinesis_service_limits = "https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html"
aws_kinesis_split_shards = "https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-split.html"
aws_s3 = "https://aws.amazon.com/s3/"
aws_s3_regions = "https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region"
aws_s3_service_limits = "https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html"
basic_auth = "https://en.wikipedia.org/wiki/Basic_access_authentication"
big_query_streaming = "https://cloud.google.com/bigquery/streaming-data-into-bigquery"
cgroups_limit_resources = "https://the.binbashtheory.com/control-resources-cgroups/"
clickhouse = "https://clickhouse.yandex/"
clickhouse_http = "https://clickhouse.yandex/docs/en/interfaces/http/"
community = "https://vector.dev/community"
configuration = "https://docs.vector.dev/usage/configuration"
crc = "https://en.wikipedia.org/wiki/Cyclic_redundancy_check"
default_configuration = "https://github.com/timberio/vector/blob/master/config/vector.toml"
docker = "https://www.docker.com/"
docker_alpine = "https://hub.docker.com/_/alpine"
dockerfile = "https://github.com/timberio/vector/blob/master/Dockerfile"
docker_hub_vector = "https://hub.docker.com/r/timberio/vector"
docs = "https://docs.vector.dev"
elasticsearch = "https://www.elastic.co/products/elasticsearch"
event_proto = "https://github.com/timberio/vector/blob/master/proto/event.proto"
exit_codes = "https://docs.rs/exitcode/1.1.2/exitcode/#constants"
globbing = "https://en.wikipedia.org/wiki/Glob_(programming)"
grok = "http://grokdebug.herokuapp.com/"
grok_debugger = "http://grokdebug.herokuapp.com/"
grok_patterns = "https://github.com/daschl/grok/tree/master/patterns"
gzip = "https://www.gzip.org/"
haproxy = "https://www.haproxy.org/"
homebrew = "https://brew.sh/"
homebrew_services = "https://github.com/Homebrew/homebrew-services"
iam_instance_profile = "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html"
initd = "https://bash.cyberciti.biz/guide//etc/init.d"
journald = "https://www.freedesktop.org/software/systemd/man/systemd-journald.service.html"
json_types = "https://en.wikipedia.org/wiki/JSON#Data_types_and_syntax"
kafka = "https://kafka.apache.org/"
kafka_partitioning_docs = "https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Partitioningandbootstrapping"
kafka_protocol = "https://kafka.apache.org/protocol"
kubernetes_limit_resources = "https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/"
leveldb = "https://github.com/google/leveldb"
log_event_source = "https://github.com/timberio/vector/blob/master/src/event/mod.rs"
lua = "https://www.lua.org/"
lua_docs = "https://www.lua.org/manual/5.3/"
lua_manual = "http://www.lua.org/manual/5.1/manual.html"
lua_require = "http://www.lua.org/manual/5.1/manual.html#pdf-require"
lua_table = "https://www.lua.org/manual/2.2/section3_3.html"
lua_types = "https://www.lua.org/manual/2.2/section3_3.html"
mailing_list = "https://vector.dev/mailing_list/"
metric_event_source = "https://github.com/timberio/vector/blob/master/src/event/metric.rs"
musl_builder_docker_image = "https://github.com/timberio/vector/blob/master/scripts/ci-docker-images/builder-x86_64-unknown-linux-musl/Dockerfile"
new_bug_report = "https://github.com/timberio/vector/issues/new?labels=Type%3A+Bug"
new_feature_request = "https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature"
new_sink = "https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature"
new_source = "https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature"
new_transform = "https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature"
nginx = "https://www.nginx.com/"
prometheus = "https://prometheus.io/"
prometheus_counter = "https://prometheus.io/docs/concepts/metric_types/#counter"
prometheus_gauge = "https://prometheus.io/docs/concepts/metric_types/#gauge"
prometheus_high_cardinality = "https://prometheus.io/docs/practices/naming/#labels"
prometheus_histogram = "https://prometheus.io/docs/concepts/metric_types/#histogram"
prometheus_histograms_guide = "https://prometheus.io/docs/practices/histograms/"
prometheus_summary = "https://prometheus.io/docs/concepts/metric_types/#summary"
prometheus_text_based_exposition_format = "https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format"
prometheus_metric_naming = "https://prometheus.io/docs/practices/naming/#metric-names"
rdkafka = "https://github.com/edenhill/librdkafka"
regex = "https://en.wikipedia.org/wiki/Regular_expression"
regex_grouping_and_flags = "https://docs.rs/regex/1.1.7/regex/#grouping-and-flags"
regex_tester = "https://regex-golang.appspot.com/assets/html/index.html"
releases = "https://github.com/timberio/vector/releases"
roadmap = "https://github.com/timberio/vector/milestones?direction=asc&sort=title&state=open"
rust = "https://www.rust-lang.org/"
rust_date_time = "https://docs.rs/chrono/0.4.0/chrono/struct.DateTime.html"
rust_grok_library = "https://github.com/daschl/grok"
rust_regex_syntax = "https://docs.rs/regex/1.1.7/regex/#syntax"
rust_target_triples  = "https://forge.rust-lang.org/platform-support.html"
splunk_hec = "http://dev.splunk.com/view/event-collector/SP-CAAAE6M"
splunk_hec_setup = "https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector"
standard_streams = "https://en.wikipedia.org/wiki/Standard_streams"
statsd_set = "https://github.com/statsd/statsd/blob/master/docs/metric_types.md#sets"
strftime_specifiers = "https://docs.rs/chrono/0.3.1/chrono/format/strftime/index.html"
syslog_5424 = "https://tools.ietf.org/html/rfc5424"
systemd = "https://www.freedesktop.org/wiki/Software/systemd/"
systemd_limit_resources = "https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html"
test_harness = "https://github.com/timberio/vector-test-harness/"
toml = "https://github.com/toml-lang/toml"
toml_array = "https://github.com/toml-lang/toml#array"
toml_table = "https://github.com/toml-lang/toml#table"
toml_types = "https://github.com/toml-lang/toml#table-of-contents"
uuidv4 = "https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_(random)"
vector_repo = "https://github.com/timberio/vector"
vector_initd_service = "https://github.com/timberio/vector/blob/master/distribution/init.d/vector"
vector_systemd_file = "https://github.com/timberio/vector/blob/master/distribution/systemd/vector.service"
vote_feature = "https://github.com/timberio/vector/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22Type%3A+New+Feature%22"
vector_chat = "https://chat.vector.dev"
website = "https://vector.dev"
yum = "http://yum.baseurl.org/"
